{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "from nptdms import TdmsFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime as datetime\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle\n",
    "from scipy.signal.windows import cosine\n",
    "from scipy.signal import sosfiltfilt, butter, detrend, sosfreqz\n",
    "from scipy.integrate import cumulative_trapezoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Functions for reading and processing borehole DAS data from EGS Collab Experiment 1\n",
    "\n",
    "def sliceIterator(lst,sliceLen):\n",
    "    \"\"\"\n",
    "    # function to generate a sliding window of slices for extracting \"sliceLen\" \n",
    "    files at a time from the main directory\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lst : list\n",
    "    lst = list of time sorted files\n",
    "    sliceLen : int\n",
    "    number of files to slice from the list\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    sliding window selection of DAS files for input into processing function    \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(lst) - sliceLen + 1):\n",
    "        files = lst[i:i + sliceLen]\n",
    "        # file1 , file2 , file3  = [*files] #unpack the 3 input files\n",
    "        # read the sampling frequencies and unpack\n",
    "        fs1,fs2,fs3 = \\\n",
    "            [*[TdmsFile.read_metadata(f).properties['SamplingFrequency[Hz]'] for f in files]]\n",
    "        if not fs1 == fs2 == fs3:\n",
    "            print(f'Sampling Frequencies of input files are different: Files skipped')\n",
    "            \n",
    "            continue\n",
    "        elif fs1 == fs2 == fs3:\n",
    "            yield lst[i:i + sliceLen]\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iDAS_timeAvg(files):\n",
    "    \"\"\"\n",
    "    function to process 30 sec tdms DAS files for low frequency strain rate\n",
    "    \n",
    "    check to make sure Fs is the same for all 3 files\n",
    "    concatenate the files together\n",
    "    taper the combined data\n",
    "    low pass the data w a cut freq of 1/30 Hz (30 sec)\n",
    "    pick the middle point?\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    files : tdms DAS data files\n",
    "    tdms (National Instrument proprietary file type) files for processing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filt_dat : pandas dataframe\n",
    "    dataframe of tapered, low-passed data from selected DAS channels\n",
    "    \"\"\"\n",
    "    # wn     = ['OT','OB','PST','PSB','PDB','PDT']\n",
    "    # ch_bot = [396, 576, 837, 982, 1150, 1329]\n",
    "    # max_z  = [59.39, 58.07, 41.01, 59.31, 58.71, 58.17]\n",
    "    OTchans = np.r_[337:456]\n",
    "    # OBchans = np.r_[518:635]\n",
    "    # PSTchans = np.r_[796:879]\n",
    "    # PSBchans = np.r_[923:1042]\n",
    "    # PDBchans = np.r_[1091:1210] # these are the channel nums of the DAS I want\n",
    "    # PDBchan = 1121\n",
    "    # PDTchans = np.r_[1271:1388]\n",
    "    \n",
    "    file1 , file2 , file3  = [*files] #unpack the 3 input files\n",
    "    # read the sampling frequencies and unpack\n",
    "    fs1,fs2,fs3 = \\\n",
    "        [*[TdmsFile.read_metadata(f).properties['SamplingFrequency[Hz]'] for f in files]] \n",
    "    # if not fs1 == fs2 == fs3:\n",
    "    #     print(f'Sampling Frequencies of input files are different: Files skipped')\n",
    "    #     pass\n",
    "        \n",
    "\n",
    "    if fs1 == fs2 == fs3 == float(1000):\n",
    "        # read them as dataframes with the built-in pandas method in nptdms\n",
    "        tdms_file1 = TdmsFile(file1).as_dataframe().iloc[:,OTchans]\n",
    "        tdms_file2 = TdmsFile(file2).as_dataframe().iloc[:,OTchans]\n",
    "        tdms_file3 = TdmsFile(file3).as_dataframe().iloc[:,OTchans]\n",
    "        # concatenate the 3 files \n",
    "        three_files = pd.concat((tdms_file1,tdms_file2,tdms_file3), axis = 0)\n",
    "        # delete large variables and garbage collect\n",
    "        del tdms_file1,tdms_file2,tdms_file3\n",
    "        gc.collect()\n",
    "        # detrend the mean and any linear trends from the data chunk \n",
    "        detrend_dat = detrend(three_files,axis=0,type='constant')\n",
    "        detrend_dat = detrend(detrend_dat,axis=0,type='linear')\n",
    "        # build the window\n",
    "        cos_win = cosine(len(three_files))\n",
    "        # employ the window/taper\n",
    "        win_dat = three_files.multiply(cos_win,axis=0)    \n",
    "        del three_files\n",
    "        gc.collect()\n",
    "      \n",
    "        # build the filter\n",
    "        Wn = 1/30 # set the low pass frequency ie 30 s here\n",
    "        sos = butter(2,Wn,output='sos',fs=fs1)\n",
    "        # view the filter response \n",
    "        # w,h = sosfreqz(sos,worN=512,fs=fs1)\n",
    "        # db = 20*np.log10(np.maximum(np.abs(h), 1e-5))\n",
    "        # apply the filter\n",
    "        filt_dat = sosfiltfilt(sos,win_dat,axis=0)    \n",
    "        mid_dat = filt_dat[math.trunc(len(filt_dat)/2)]\n",
    "        # mid_dat = detrend_dat[math.trunc(len(detrend_dat)/2)]\n",
    "        return mid_dat\n",
    "\n",
    "    elif fs1 == fs2 == fs3 == float(10000):\n",
    "        # read them as dataframes with the built-in pandas method in nptdms\n",
    "        tdms_file1 = TdmsFile(file1).as_dataframe().iloc[:,OTchans]\n",
    "        tdms_file2 = TdmsFile(file2).as_dataframe().iloc[:,OTchans]\n",
    "        tdms_file3 = TdmsFile(file3).as_dataframe().iloc[:,OTchans]\n",
    "        # concatenate the 3 files \n",
    "        three_files = pd.concat((tdms_file1,tdms_file2,tdms_file3), axis = 0)\n",
    "        # delete large variables and garbage collect\n",
    "        del tdms_file1,tdms_file2,tdms_file3\n",
    "        gc.collect()\n",
    "        # detrend the mean and any linear trends from the data chunk \n",
    "        detrend_dat = detrend(three_files,axis=0,type='constant')\n",
    "        detrend_dat = detrend(detrend_dat,axis=0,type='linear')\n",
    "        # build the window\n",
    "        cos_win = cosine(len(three_files))\n",
    "        # employ the window/taper\n",
    "        win_dat = three_files.multiply(cos_win,axis=0)    \n",
    "        del three_files\n",
    "        gc.collect()\n",
    "        # build the filter\n",
    "        Wn = 1/30 # set the low pass frequency ie 30 s here\n",
    "        sos = butter(2,Wn,output='sos',fs=fs1)\n",
    "        # apply the filter\n",
    "        filt_dat = sosfiltfilt(sos,win_dat,axis=0)\n",
    "        mid_dat = filt_dat[math.trunc(len(filt_dat)/2)]\n",
    "        # mid_dat = detrend_dat[math.trunc(len(detrend_dat)/2)]\n",
    "        return mid_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to cd into the directory where the data live bc mp.pool doesn't \n",
    "# work via interactive interpreters\n",
    "start = time.time()\n",
    "dirpath = '/data1/parker/EGS_iDAS/20180522/'\n",
    "os.chdir(dirpath)\n",
    "postfix='tdms'\n",
    "file_list = [f for f in sorted(os.listdir(dirpath)) if (postfix in f)]\n",
    "slice_len = 3\n",
    "num_proc = 12\n",
    "pool = mp.Pool(processes = num_proc)\n",
    "\n",
    "# for files in sliceIterator(file_list,slice_len):\n",
    "#     proc = pool.apply_async(iDAS_timeAvg,args=[files])\n",
    "proc = [pool.apply_async(iDAS_timeAvg,args=(files,)) for files in sliceIterator(file_list,slice_len)]    \n",
    "results = [p.get() for p in proc]\n",
    "print(time.time()-start)\n",
    "results = pd.DataFrame(results)\n",
    "results.to_csv(r'/home/spri902/EGS_Collab/4850/results/maystim/processed_DAS/lpFilter/wellOT/20180522_allchan.txt',\\\n",
    "                      header=True,index=None,sep=',',mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put separate dat files back together\n",
    "os.chdir('/home/spri902/EGS_Collab/4850/results/maystim/processed_DAS/lpFilter/wellOB')\n",
    "datFiles = sorted(os.listdir('/home/spri902/EGS_Collab/4850/results/maystim/processed_DAS/lpFilter/wellOB'))\n",
    "# # df_raw = pd.concat((pd.read_csv(f,header=0,sep=',') for f in datFiles if f.endswith('r.txt')),axis=0)\n",
    "# # df_all = pd.concat((pd.read_csv(f,header=0,sep=',') for f in datFiles if f.endswith('n.txt')),axis=0)\n",
    "df_full = pd.concat((pd.read_csv(f,header=0,sep=',') for f in datFiles if f.endswith('allchan.txt')),axis=0)\n",
    "# df_all.reset_index(drop=True,inplace=True)\n",
    "# df_raw.reset_index(drop=True,inplace=True)\n",
    "df_full.reset_index(drop=True,inplace=True)\n",
    "# # ndf = [df_all[col].str.split(' ',expand=True) for col in df_all.columns]\n",
    "# # ndf = pd.concat(ndf,axis=1)\n",
    "df_full.to_pickle('maystim22_26_combined_full')\n",
    "wn     = ['OT','OB','PST','PSB','PDB','PDT']\n",
    "nfile_list = sorted(os.walk('/data1/parker/EGS_iDAS'))\n",
    "nfile_list = nfile_list[1:]\n",
    "# file_list = file_list[1:]\n",
    "nfile_list = [group[2] for group in nfile_list]\n",
    "nfile_list = [item for sublist in nfile_list for item in sublist]\n",
    "# [file_list.append(f) for f in nfile_list]\n",
    "fd = [name.split(\"_\") for name in nfile_list]\n",
    "fl = [fd[file][2].split(\".\") for file in range(len(fd))]\n",
    "fl = [el[0] for el in fl]\n",
    "dates = [datetime.strptime(d,'%y%m%d%H%M%S') for d in sorted(fl)]\n",
    "# dates = dates[1:-1]\n",
    "xlims = mdates.date2num(dates)  \n",
    "chans=np.linspace(0,df_full.shape[0] - 1,df_full.shape[0]).astype(int)\n",
    "ch_bot = [396, 576, 837, 982, 1150, 1329]\n",
    "# remove_dates = pd.read_csv('/home/spri902/EGS_Collab/4850/results/maystim/processed_DAS/skipFlies.txt',sep=',',header=None)\n",
    "\n",
    "ind2rem = [0, 90,   91, 257, 258, 1571, 1572, 3082, 3083, 5085, 5086, 5599, 5600, 5961, 5962, 7623, 7624, 8841, 8842, 9562]\n",
    "for index in sorted(ind2rem,reverse=True):\n",
    "    del dates[index]\n",
    "dates = pd.DataFrame(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('obspy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58dcc157cae2c70039da6c5c30e2e6a4bafd07ea2e2d931bc31227249bf45b89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
